{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Performance Bottlenecks",
   "id": "884670690304765c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Naive Distinct Value Tracking",
   "id": "2af3db4426314b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def process_ids(ids):\n",
    "    unique_ids = []\n",
    "    duplicates = 0\n",
    "    for id_val in ids:\n",
    "        if id_val in unique_ids:\n",
    "            duplicates += 1\n",
    "        else:\n",
    "            unique_ids.append(id_val)\n",
    "    return unique_ids, duplicates\n",
    "\n",
    "def main():\n",
    "    # Simulate 100,000 IDs with some duplicates\n",
    "    random.seed(42)\n",
    "    ids = [random.randint(1, 100000) for _ in range(100000)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    unique, dups = process_ids(ids)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Found {len(unique)} unique IDs and {dups} duplicates in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "34da7ae64cc3647d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note when I got an AI-generated recommendation ho",
   "id": "a759360453ad9694"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def process_ids(ids):\n",
    "    seen_ids = set()\n",
    "    unique_ids = []\n",
    "    duplicates = 0\n",
    "    for id_val in ids:\n",
    "        if id_val in seen_ids:  # Efficient O(1) lookup on set\n",
    "            duplicates += 1\n",
    "        else:\n",
    "            seen_ids.add(id_val)\n",
    "            unique_ids.append(id_val)  # Maintain order if needed\n",
    "    return unique_ids, duplicates\n",
    "\n",
    "def main():\n",
    "    # Simulate 100,000 IDs with some duplicates\n",
    "    random.seed(42)\n",
    "    ids = [random.randint(1, 100000) for _ in range(100000)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    unique, dups = process_ids(ids)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Found {len(unique)} unique IDs and {dups} duplicates in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "65f36b78a4d80d70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `seen_ids` list was completely redundant to the `seen_ids`.",
   "id": "9506254c69034fb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def process_ids(ids):\n",
    "    unique_ids = set()\n",
    "    duplicates = 0\n",
    "    for id_val in ids:\n",
    "        if id_val in unique_ids:  # Efficient O(1) lookup on set\n",
    "            duplicates += 1\n",
    "        else:\n",
    "            unique_ids.add(id_val)\n",
    "    return unique_ids, duplicates\n",
    "\n",
    "def main():\n",
    "    # Simulate 100,000 IDs with some duplicates\n",
    "    random.seed(42)\n",
    "    ids = [random.randint(1, 100000) for _ in range(100000)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    unique, dups = process_ids(ids)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Found {len(unique)} unique IDs and {dups} duplicates in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "461eb592a556afe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Nested Loop Bottlenecks",
   "id": "6857407f333d51a"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "def process_users(user_data):\n",
    "    # Simulate a large dataset of users\n",
    "    filtered_users = []\n",
    "\n",
    "\n",
    "    for user in user_data:\n",
    "        # Check if user is active and has a valid email\n",
    "        email = user['email']\n",
    "        is_active = user['active']\n",
    "\n",
    "        #  string search for email validation\n",
    "        if is_active and '@' in email:\n",
    "            # checking duplicates\n",
    "            for existing_user in filtered_users:\n",
    "                if existing_user['email'] == email:\n",
    "                    break\n",
    "            else:\n",
    "\n",
    "                processed_user = {\n",
    "                    'email': email.lower() + '_processed',\n",
    "                    'name': user['name'].upper(),\n",
    "                    'id': str(user['id'])\n",
    "                }\n",
    "                filtered_users.append(processed_user)\n",
    "\n",
    "    return filtered_users\n",
    "\n",
    "def main():\n",
    "    # Simulate a large dataset\n",
    "    users = [\n",
    "        {'id': i, 'email': f'user{i}@example.com', 'name': f'User {i}', 'active': i % 2 == 0}\n",
    "        for i in range(100000)\n",
    "    ]\n",
    "\n",
    "    # Process the users\n",
    "    result = process_users(users)\n",
    "    print(f\"Processed {len(result)} users\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "178bcd8f794eff1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def process_users(user_data):\n",
    "    filtered_users = []\n",
    "    seen_emails = set()  # Track duplicates\n",
    "\n",
    "    for user in user_data:\n",
    "        email = user['email']\n",
    "        if user['active'] and '@' in email and email not in seen_emails:\n",
    "            processed_user = {\n",
    "                'email': email.lower() + '_processed',\n",
    "                'name': user['name'].upper(),\n",
    "                'id': str(user['id'])\n",
    "            }\n",
    "            filtered_users.append(processed_user)\n",
    "            seen_emails.add(email)  # Mark as seen\n",
    "\n",
    "    return filtered_users\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Simulate a large dataset\n",
    "    users = [\n",
    "        {'id': i, 'email': f'user{i}@example.com', 'name': f'User {i}', 'active': i % 2 == 0}\n",
    "        for i in range(100000)\n",
    "    ]\n",
    "\n",
    "    # Process the users\n",
    "    result = process_users(users)\n",
    "    print(f\"Processed {len(result)} users\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "c556b8175791e33b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SQL Optimization",
   "id": "a74ada28c5600944"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's start with a common mistake that is easy to do in Python SQL. Let's say I want to insert the next 10_000 dates into a `CALENDAR` table that is a single column of dates. Why is this so slow?",
   "id": "ea350f89841f0e21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T13:39:35.627559Z",
     "start_time": "2025-08-17T13:39:35.606158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sqlite3\n",
    "\n",
    "for _ in range(10_000):\n",
    "    conn = sqlite3.connect(\"company_operations.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"INSERT INTO CALENDAR (CALENDAR_DATE) VALUES\n",
    "                   ((SELECT DATE(MAX(CALENDAR_DATE),'+1 day') FROM CALENDAR))\n",
    "                   \"\"\")\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n"
   ],
   "id": "688239a154939545",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "conn = sqlite3.connect(\"company_operations.db\")"
   ],
   "id": "c928bdd2e524eb43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's pretend for a moment that this SQL query below took a very long time. Let's see what it recommends.",
   "id": "ee94f0154de942a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T13:12:43.117717Z",
     "start_time": "2025-08-17T13:12:43.111305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT * FROM WEATHER_MONITOR\n",
    "WHERE REPORT_DATE = '2021-05-05'\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "pd.read_sql(sql, conn)\n",
    "end = time.time()\n",
    "print(f\"Query took {end - start:.4f} seconds\")\n"
   ],
   "id": "d97a7b15ba0d9426",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query took 0.0029 seconds\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "But keep in mind, if the AI recommends an index and other solutions, there are gotchas like write speed going down substantially. These are nuances that you cannot pick up from vibe coding, and is why you should know the subject matter you are prompting.\n",
    "\n",
    "Here is another example. Let's try to optimize this query."
   ],
   "id": "129ff16fcff73e01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T13:14:25.320330Z",
     "start_time": "2025-08-17T13:14:25.275974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"company_operations.db\")\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "CUSTOMER_ID,\n",
    "ORDER_DATE,\n",
    "QUANTITY,\n",
    "(SELECT AVG(QUANTITY)\n",
    " FROM CUSTOMER_ORDER co3\n",
    " WHERE co3.CUSTOMER_ID = co1.CUSTOMER_ID) as avg_customer_quantity\n",
    "FROM CUSTOMER_ORDER co1\n",
    "ORDER BY ORDER_DATE\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "pd.read_sql(sql, conn)\n",
    "end = time.time()\n",
    "print(f\"Query took {end - start:.4f} seconds\")"
   ],
   "id": "b64d096294d4a10f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      CUSTOMER_ID  ORDER_DATE  QUANTITY  avg_customer_quantity\n",
       "0               9  2021-01-01        20             110.970149\n",
       "1               5  2021-01-01       110             104.700855\n",
       "2               3  2021-01-01       120             103.451327\n",
       "3               6  2021-01-01       200             100.977444\n",
       "4               2  2021-01-01        60             103.423423\n",
       "...           ...         ...       ...                    ...\n",
       "1185            9  2021-03-31        70             110.970149\n",
       "1186            5  2021-03-31       140             104.700855\n",
       "1187           10  2021-03-31        80             100.080000\n",
       "1188            9  2021-03-31        20             110.970149\n",
       "1189            8  2021-03-31       100             102.156863\n",
       "\n",
       "[1190 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>ORDER_DATE</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>avg_customer_quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>20</td>\n",
       "      <td>110.970149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>110</td>\n",
       "      <td>104.700855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>120</td>\n",
       "      <td>103.451327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>200</td>\n",
       "      <td>100.977444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>60</td>\n",
       "      <td>103.423423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>9</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>70</td>\n",
       "      <td>110.970149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>5</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>140</td>\n",
       "      <td>104.700855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>10</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>80</td>\n",
       "      <td>100.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>9</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>20</td>\n",
       "      <td>110.970149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>8</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>100</td>\n",
       "      <td>102.156863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1190 rows Ã— 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ideally, we'd like to get a windowing function once the AI recommends a fix. But it may settle for a common table expression or derived table too. This again shows that the AI may propose something that works but may be suboptimal.",
   "id": "c8dce14e27b5673"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
