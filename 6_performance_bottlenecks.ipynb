{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884670690304765c",
   "metadata": {},
   "source": [
    "# Performance Bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccda1be0cc24891",
   "metadata": {},
   "source": [
    "Let's look at examples of how AI can help us identify performance bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3db4426314b6",
   "metadata": {},
   "source": [
    "## Naive Distinct Value Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da7ae64cc3647d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:18:40.558762Z",
     "start_time": "2025-08-25T12:18:40.515032Z"
    }
   },
   "outputs": [],
   "source": [
    "import time  # Import time module for measuring execution time\n",
    "import random  # Import random module for generating random numbers\n",
    "\n",
    "def process_ids(ids):\n",
    "    # Process a list of IDs to identify unique IDs and count duplicates.\n",
    "    unique_ids = []  # Initialize an empty list to store unique IDs\n",
    "    duplicates = 0  # Initialize a counter for duplicates\n",
    "    for id_val in ids:  # Iterate over each ID in the input list\n",
    "        if id_val in unique_ids:  # Check if the ID is already in the unique list\n",
    "            duplicates += 1  # If yes, increment the duplicate count\n",
    "        else:  # If not, it's a new unique ID\n",
    "            unique_ids.append(id_val)  # Add it to the unique list\n",
    "    return unique_ids, duplicates  # Return the unique IDs list and duplicate count\n",
    "\n",
    "# Simulate 100,000 IDs with some duplicates\n",
    "random.seed(42)  # Set a fixed seed for reproducibility of random numbers\n",
    "ids = [random.randint(1, 100000) for _ in range(100000)]  # Generate a list of 100,000 random integers between 1 and 100,000\n",
    "\n",
    "start_time = time.time()  # Record the start time before processing\n",
    "unique, dups = process_ids(ids)  # Call the function to process the IDs and get unique IDs and duplicate count\n",
    "end_time = time.time()  # Record the end time after processing\n",
    "\n",
    "# Print the results, including the number of unique IDs, duplicates, and execution time formatted to 2 decimal places\n",
    "print(f\"Found {len(unique)} unique IDs and {dups} duplicates in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a77245-9647-43e1-b8c7-2650e8395584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def process_ids(ids):\n",
    "    unique_ids_set = set()  # Use a set for fast membership checking\n",
    "    duplicates = 0\n",
    "    for id_val in ids:\n",
    "        if id_val in unique_ids_set:\n",
    "            duplicates += 1\n",
    "        else:\n",
    "            unique_ids_set.add(id_val)\n",
    "    unique_ids = list(unique_ids_set)  # Convert back to list if needed\n",
    "    return unique_ids, duplicates\n",
    "\n",
    "# Simulate 100,000 IDs with some duplicates\n",
    "random.seed(42)\n",
    "ids = [random.randint(1, 100000) for _ in range(100000)]\n",
    "\n",
    "start_time = time.time()\n",
    "unique, dups = process_ids(ids)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Found {len(unique)} unique IDs and {dups} duplicates in {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c945d8f-826e-473f-88c7-cafddb961a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "ids = [random.randint(1, 100000) for _ in range(100_000)]\n",
    "\n",
    "start_time = time.time()\n",
    "ids_array = np.array(ids)\n",
    "unique_ids = np.unique(ids_array)       # Returns sorted unique IDs\n",
    "duplicates = len(ids) - len(unique_ids)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Found {len(unique_ids)} unique IDs and {duplicates} duplicates in {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8922fce5-dbfc-4b42-92d6-f536bff95af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "ids = [random.randint(1, 100000) for _ in range(100_000)]\n",
    "\n",
    "start_time = time.time()\n",
    "ids_series = pd.Series(ids)\n",
    "unique_ids = ids_series.unique()\n",
    "duplicates = len(ids) - len(unique_ids)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Found {len(unique_ids)} unique IDs and {duplicates} duplicates in {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e786f0999e121",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:48px; line-height:1.2;\">\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "â†“\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a759360453ad9694",
   "metadata": {},
   "source": [
    "Note when I got an AI-generated recommendation, it made some unnecessary code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f36b78a4d80d70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:17:54.580803Z",
     "start_time": "2025-08-25T12:17:54.533786Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def process_ids(ids):\n",
    "    seen_ids = set()\n",
    "    unique_ids = []\n",
    "    duplicates = 0\n",
    "    for id_val in ids:\n",
    "        if id_val in seen_ids:  # Efficient O(1) lookup on set\n",
    "            duplicates += 1\n",
    "        else:\n",
    "            seen_ids.add(id_val)\n",
    "            unique_ids.append(id_val)  # Maintain order if needed\n",
    "    return unique_ids, duplicates\n",
    "\n",
    "# Simulate 100,000 IDs with some duplicates\n",
    "random.seed(42)\n",
    "ids = [random.randint(1, 100000) for _ in range(100000)]\n",
    "\n",
    "start_time = time.time()\n",
    "unique, dups = process_ids(ids)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Found {len(unique)} unique IDs and {dups} duplicates in {end_time - start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9506254c69034fb4",
   "metadata": {},
   "source": [
    "The `seen_ids` list was completely redundant to the `seen_ids`. It was better I make a modificiation to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461eb592a556afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def process_ids(ids):\n",
    "    unique_ids = set()\n",
    "    duplicates = 0\n",
    "    for id_val in ids:\n",
    "        if id_val in unique_ids:  # Efficient O(1) lookup on set\n",
    "            duplicates += 1\n",
    "        else:\n",
    "            unique_ids.add(id_val)\n",
    "    return unique_ids, duplicates\n",
    "\n",
    "# Simulate 100,000 IDs with some duplicates\n",
    "random.seed(42)\n",
    "ids = [random.randint(1, 100000) for _ in range(100000)]\n",
    "\n",
    "start_time = time.time()\n",
    "unique, dups = process_ids(ids)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Found {len(unique)} unique IDs and {dups} duplicates in {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857407f333d51a",
   "metadata": {},
   "source": [
    "## Nested Loop Bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7f4660a488fe91",
   "metadata": {},
   "source": [
    "This one is gnarly, and it's complex and slow. It captures active users with valid emails from a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178bcd8f794eff1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:24:51.821121Z",
     "start_time": "2025-08-25T12:24:36.622514Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def process_users(user_data):\n",
    "    # Process a list of user data to filter and transform active users with valid emails,\n",
    "    # ensuring no duplicate emails are included in the result.\n",
    "\n",
    "    filtered_users = []  # Initialize an empty list to store processed unique users\n",
    "\n",
    "    for user in user_data:  # Iterate over each user dictionary in the input list\n",
    "        # Extract relevant fields for clarity and checks\n",
    "        email = user['email']  # Get the user's email\n",
    "        is_active = user['active']  # Get the user's active status (boolean)\n",
    "\n",
    "        # Validate if the user is active and the email contains '@' as a simple check for validity\n",
    "        if is_active and '@' in email:\n",
    "            # Check for duplicates by iterating through already filtered users\n",
    "            for existing_user in filtered_users:\n",
    "                if existing_user['email'] == email:  # Compare emails to detect duplicates\n",
    "                    break  # If a duplicate is found, skip adding this user\n",
    "            else:  # This else clause executes only if no break occurred (no duplicate found)\n",
    "                # Create a new processed user dictionary\n",
    "                processed_user = {\n",
    "                    'email': email.lower() + '_processed',  # Lowercase the email and append '_processed'\n",
    "                    'name': user['name'].upper(),  # Uppercase the user's name\n",
    "                    'id': str(user['id'])  # Convert the ID to a string\n",
    "                }\n",
    "                filtered_users.append(processed_user)  # Add the processed user to the list\n",
    "\n",
    "    return filtered_users  # Return the list of processed unique users\n",
    "\n",
    "\n",
    "# Simulate a large dataset of 100,000 users for testing\n",
    "# Each user has an ID from 0 to 99999, an email like 'userX@example.com',\n",
    "# a name like 'User X', and active status alternating (even IDs are active)\n",
    "users = [\n",
    "    {'id': i, 'email': f'user{i}@example.com', 'name': f'User {i}', 'active': i % 2 == 0}\n",
    "    for i in range(100000)\n",
    "]\n",
    "\n",
    "start = time.time()\n",
    "# Process the simulated users using the function\n",
    "result = process_users(users)\n",
    "end = time.time()\n",
    "\n",
    "# Print the number of processed users (should be half of 100,000 since only even IDs are active, and no duplicates)\n",
    "print(f\"Processed {len(result)} users in {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd4720-2e12-4fab-afc6-a01c1cb7b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def process_users(user_data):\n",
    "    processed_users = []\n",
    "    seen_emails = set()  # Track already processed emails\n",
    "\n",
    "    for user in user_data:\n",
    "        email = user['email']\n",
    "        if user['active'] and '@' in email and email not in seen_emails:\n",
    "            seen_emails.add(email)\n",
    "            processed_users.append({\n",
    "                'email': email.lower() + '_processed',\n",
    "                'name': user['name'].upper(),\n",
    "                'id': str(user['id'])\n",
    "            })\n",
    "    \n",
    "    return processed_users\n",
    "\n",
    "# Simulate 100,000 users\n",
    "users = [\n",
    "    {'id': i, 'email': f'user{i}@example.com', 'name': f'User {i}', 'active': i % 2 == 0}\n",
    "    for i in range(100_000)\n",
    "]\n",
    "\n",
    "start = time.time()\n",
    "result = process_users(users)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Processed {len(result)} users in {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ff8b1-9ccf-4c5b-a7d1-efb64432e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Simulate 100,000 users\n",
    "users = pd.DataFrame({\n",
    "    'id': range(100_000),\n",
    "    'email': [f'user{i}@example.com' for i in range(100_000)],\n",
    "    'name': [f'User {i}' for i in range(100_000)],\n",
    "    'active': [i % 2 == 0 for i in range(100_000)]\n",
    "})\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Filter active users with valid emails\n",
    "df = users[users['active'] & users['email'].str.contains('@')]\n",
    "\n",
    "# Drop duplicate emails\n",
    "df = df.drop_duplicates(subset='email')\n",
    "\n",
    "# Apply transformations\n",
    "df['email'] = df['email'].str.lower() + '_processed'\n",
    "df['name'] = df['name'].str.upper()\n",
    "df['id'] = df['id'].astype(str)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Processed {len(df)} users in {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc54812f8e44837b",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:48px; line-height:1.2;\">\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "â†“\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3abed922b372db1",
   "metadata": {},
   "source": [
    "We can simplify this a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556b8175791e33b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:25:20.325752Z",
     "start_time": "2025-08-25T12:25:20.271584Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "def process_users(user_data):\n",
    "    filtered_users = []\n",
    "    seen_emails = set()  # Track duplicates\n",
    "\n",
    "    for user in user_data:\n",
    "        email = user['email']\n",
    "        if user['active'] and '@' in email and email not in seen_emails:\n",
    "            processed_user = {\n",
    "                'email': email.lower() + '_processed',\n",
    "                'name': user['name'].upper(),\n",
    "                'id': str(user['id'])\n",
    "            }\n",
    "            filtered_users.append(processed_user)\n",
    "            seen_emails.add(email)  # Mark as seen\n",
    "\n",
    "    return filtered_users\n",
    "\n",
    "\n",
    "# Simulate a large dataset\n",
    "users = [\n",
    "    {'id': i, 'email': f'user{i}@example.com', 'name': f'User {i}', 'active': i % 2 == 0}\n",
    "    for i in range(100000)\n",
    "]\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "# Process the simulated users using the function\n",
    "result = process_users(users)\n",
    "end = time.time()\n",
    "\n",
    "# Print the number of processed users (should be half of 100,000 since only even IDs are active, and no duplicates)\n",
    "print(f\"Processed {len(result)} users in {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ada28c5600944",
   "metadata": {},
   "source": [
    "## SQL Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea350f89841f0e21",
   "metadata": {},
   "source": [
    "Let's start with a common mistake that is easy to do in Python SQL. Let's say I want to insert the next 10_000 dates into a `CALENDAR` table that is a single column of dates. Why is this so slow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688239a154939545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T00:10:32.682449Z",
     "start_time": "2025-08-25T00:10:30.503141Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for _ in range(10_000):\n",
    "    conn = sqlite3.connect(\"company_operations.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"INSERT INTO CALENDAR (CALENDAR_DATE) VALUES\n",
    "                   ((SELECT DATE(MAX(CALENDAR_DATE),'+1 day') FROM CALENDAR))\n",
    "                   \"\"\")\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "end = time.end()\n",
    "\n",
    "print(f\"Finished in {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f564b9-9b05-44c4-bf04-81838d181d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "conn = sqlite3.connect(\"company_operations.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for _ in range(10_000):\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO CALENDAR (CALENDAR_DATE)\n",
    "        VALUES ((SELECT DATE(MAX(CALENDAR_DATE), '+1 day') FROM CALENDAR))\n",
    "    \"\"\")\n",
    "\n",
    "conn.commit()  # Commit once at the end\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Finished in {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c99967-40cb-4226-9f84-26baaeafac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Connect once\n",
    "conn = sqlite3.connect(\"company_operations.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Find the last date in the table\n",
    "cursor.execute(\"SELECT MAX(CALENDAR_DATE) FROM CALENDAR\")\n",
    "last_date_str = cursor.fetchone()[0]  # e.g., '2025-08-25'\n",
    "last_date = datetime.strptime(last_date_str, \"%Y-%m-%d\")\n",
    "\n",
    "# Generate 10,000 new consecutive dates\n",
    "new_dates = [(last_date + timedelta(days=i+1),) for i in range(10_000)]\n",
    "\n",
    "# Bulk insert all at once\n",
    "cursor.executemany(\"INSERT INTO CALENDAR (CALENDAR_DATE) VALUES (?)\", new_dates)\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Inserted 10,000 dates in {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94f0154de942a3",
   "metadata": {},
   "source": [
    "Let's pretend for a moment that this SQL query below took a very long time and the table is quite large in number of records. Let's see what it recommends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a7b15ba0d9426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:29:50.496547Z",
     "start_time": "2025-08-25T12:29:50.481561Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "conn = sqlite3.connect(\"company_operations.db\")\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT * FROM WEATHER_MONITOR\n",
    "WHERE REPORT_DATE = '2021-05-05'\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "pd.read_sql(sql, conn)\n",
    "end = time.time()\n",
    "print(f\"Query took {end - start:.4f} seconds\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ff16fcff73e01",
   "metadata": {},
   "source": [
    "But keep in mind, if the AI recommends an index and other solutions, there are gotchas like write speed going down substantially. These are nuances that you cannot pick up from vibe coding, and is why you should know the subject matter you are prompting.\n",
    "\n",
    "Here is another example. Let's try to optimize this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d096294d4a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "conn = sqlite3.connect(\"company_operations.db\")\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "CUSTOMER_ID,\n",
    "ORDER_DATE,\n",
    "QUANTITY,\n",
    "(SELECT AVG(QUANTITY)\n",
    " FROM CUSTOMER_ORDER co3\n",
    " WHERE co3.CUSTOMER_ID = co1.CUSTOMER_ID) as avg_customer_quantity\n",
    "FROM CUSTOMER_ORDER co1\n",
    "ORDER BY ORDER_DATE\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "pd.read_sql(sql, conn)\n",
    "end = time.time()\n",
    "print(f\"Query took {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14fb629-51c9-48bc-8a7a-ce4cce3ae6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "conn = sqlite3.connect(\"company_operations.db\")\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "    co1.CUSTOMER_ID,\n",
    "    co1.ORDER_DATE,\n",
    "    co1.QUANTITY,\n",
    "    avg_per_customer.avg_quantity AS avg_customer_quantity\n",
    "FROM CUSTOMER_ORDER co1\n",
    "JOIN (\n",
    "    SELECT CUSTOMER_ID, AVG(QUANTITY) AS avg_quantity\n",
    "    FROM CUSTOMER_ORDER\n",
    "    GROUP BY CUSTOMER_ID\n",
    ") AS avg_per_customer\n",
    "ON co1.CUSTOMER_ID = avg_per_customer.CUSTOMER_ID\n",
    "ORDER BY co1.ORDER_DATE\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "pd.read_sql(sql, conn)\n",
    "end = time.time()\n",
    "print(f\"Query took {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dce14e27b5673",
   "metadata": {},
   "source": [
    "Ideally, we'd like to get a windowing function once the AI recommends a fix. But it may settle for a common table expression or derived table too. This again shows that the AI may propose something that works but may be suboptimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a4ef06c9e8d019",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a5b0a4ec8d2bcb",
   "metadata": {},
   "source": [
    "Let's see how we can have AI convert our code to be vectorized for numerical operations. This will make our code much faster and help us learn NumPy equivalents to vanilla python loop operations.\n",
    "\n",
    "Here is a simple summation of the numbers 1 through 300 million. It takes nearly 10 seconds to do. Let's see how AI suggests we fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6143393305aad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "total = 0\n",
    "for i in range(300_000_000):\n",
    "    total += i\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Total: {total} in {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa462ccdf7bccb9b",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:48px; line-height:1.2;\">\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "â†“\n",
    "</div>\n",
    "\n",
    "\n",
    "Here is what ChatGPT suggested I do. It takes less than a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5decf99c31e94e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "total = np.sum(np.arange(300_000_000))\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Total: {total} in {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241c32629c1346",
   "metadata": {},
   "source": [
    "Calculating this average took nearly 7 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d3ad61581fbc41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "x = [i for i in range(300_000_000)]\n",
    "\n",
    "mean = sum(x) / len(x)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Mean: {mean} in {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c483111f13467",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:48px; line-height:1.2;\">\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "â†“\n",
    "</div>\n",
    "Below, the AI-generated response took less than half a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28256dfef19f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "x = np.arange(300_000_000)\n",
    "mean = np.mean(x)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Mean: {mean} in {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1605fa6fcac136",
   "metadata": {},
   "source": [
    "Now here is something painful. This operation takes nearly 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391de5ecc9ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "x = list(range(300_000_000))\n",
    "y = list(range(300_000_000))\n",
    "z = []\n",
    "for i in range(len(x)):\n",
    "    z.append(x[i] + y[i])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Finished in {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4998d587712677ea",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:48px; line-height:1.2;\">\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "â†“\n",
    "</div>\n",
    "\n",
    "But the AI-generated optimization takes less than 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e4a8c-b371-4707-b86e-4c1e7d61df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "x = np.arange(300_000_000, dtype=np.int64)\n",
    "y = np.arange(300_000_000, dtype=np.int64)\n",
    "z = x + y  # Vectorized addition\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Finished in {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b84d062116c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "x = np.arange(300_000_000)\n",
    "y = np.arange(300_000_000)\n",
    "z = x + y\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Finished in {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b82a571e9025",
   "metadata": {},
   "source": [
    "Here we try to calculate mean, varaince, standard deviation from a list of numbers to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e37d0efece2a151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:32:28.122712Z",
     "start_time": "2025-08-25T12:32:27.998697Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "data = [i * 0.1 for i in range(1_000_000)]  # Simulated dataset\n",
    "mean = sum(data) / len(data)\n",
    "variance = sum((x - mean) ** 2 for x in data) / len(data)\n",
    "std_dev = math.sqrt(variance)\n",
    "normalized = []\n",
    "for val in data:\n",
    "    normalized.append((val - mean) / std_dev)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Finished in {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05a643a7149ab",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:48px; line-height:1.2;\">\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "â†“\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211bf7435b6e75a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:32:30.675246Z",
     "start_time": "2025-08-25T12:32:30.652636Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "data = np.arange(1_000_000) * 0.1  # Simulated dataset\n",
    "normalized = (data - np.mean(data)) / np.std(data)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Finished in {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135a580f4010116",
   "metadata": {},
   "source": [
    "## Computing Pairwise Euclidean Distances for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec47def21126a81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "points = [[i, i+1] for i in range(10_000)]  # List of [x, y] points\n",
    "distances = []\n",
    "for i in range(len(points)):\n",
    "    row = []\n",
    "    for j in range(len(points)):\n",
    "        dx = points[i][0] - points[j][0]\n",
    "        dy = points[i][1] - points[j][1]\n",
    "        row.append(math.sqrt(dx**2 + dy**2))\n",
    "    distances.append(row)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Finished in {:.4f} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa943f-2309-4cc3-83ae-10e344427be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "points = np.array([[i, i+1] for i in range(10_000)], dtype=np.float64)\n",
    "\n",
    "start = time.time()\n",
    "distances = cdist(points, points, metric='euclidean')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Finished in {:.4f} seconds\".format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663383571228ae25",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:48px; line-height:1.2;\">\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "â†“\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87085c853049f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "points = np.arange(10_000)[:, np.newaxis] * np.array([1, 1]) + np.array([0, 1])  # Array of shape (10000, 2)\n",
    "diffs = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n",
    "distances = np.sqrt(np.sum(diffs**2, axis=-1))\n",
    "end = time.time()\n",
    "\n",
    "print(\"Finished in {:.4f} seconds\".format(end - start))\n",
    "# Alternatively, for even faster: use scipy.spatial.distance.cdist(points, points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb329a0ad0e7bd",
   "metadata": {},
   "source": [
    "## EXERCISE\n",
    "\n",
    "Try to speed up this time series operation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83186b7a83382caa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:41:31.516005Z",
     "start_time": "2025-08-25T12:41:30.746151Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "prices = [i * 0.01 for i in range(10_000_000)]  # Time-series prices\n",
    "volumes = [i % 1000 for i in range(10_000_000)]  # Volumes\n",
    "high_volume_prices = []\n",
    "for i in range(len(prices)):\n",
    "    if volumes[i] > 500:\n",
    "        high_volume_prices.append(prices[i])\n",
    "avg_price = sum(high_volume_prices) / len(high_volume_prices) if high_volume_prices else 0\n",
    "end = time.time()\n",
    "\n",
    "print(\"Finished in {:.4f} seconds\".format(end - start))\n",
    "print(f\"Average high-volume price: {avg_price:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2850238-022a-494f-b49a-b41aa567bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Create NumPy arrays\n",
    "prices = np.arange(10_000_000, dtype=np.float64) * 0.01\n",
    "volumes = np.arange(10_000_000) % 1000\n",
    "\n",
    "# Filter prices where volume > 500\n",
    "mask = volumes > 500\n",
    "high_volume_prices = prices[mask]\n",
    "\n",
    "# Compute average\n",
    "avg_price = high_volume_prices.mean() if high_volume_prices.size > 0 else 0\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Finished in {end - start:.4f} seconds\")\n",
    "print(f\"Average high-volume price: {avg_price:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b71016aef7668",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:48px; line-height:1.2;\">\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "â†“\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229df3a1e1c5abac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:41:33.022046Z",
     "start_time": "2025-08-25T12:41:32.874332Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "prices = np.arange(10_000_000) * 0.01\n",
    "volumes = np.arange(10_000_000) % 1000\n",
    "mask = volumes > 500\n",
    "high_volume_prices = prices[mask]\n",
    "avg_price = np.mean(high_volume_prices) if len(high_volume_prices) > 0 else 0\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Finished in {:.4f} seconds\".format(end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
