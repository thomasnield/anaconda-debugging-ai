{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Performance Bottlenecks",
   "id": "884670690304765c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Naive Distinct Value Tracking",
   "id": "2af3db4426314b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def process_ids(ids):\n",
    "    unique_ids = []\n",
    "    duplicates = 0\n",
    "    for id_val in ids:\n",
    "        if id_val in unique_ids:\n",
    "            duplicates += 1\n",
    "        else:\n",
    "            unique_ids.append(id_val)\n",
    "    return unique_ids, duplicates\n",
    "\n",
    "def main():\n",
    "    # Simulate 100,000 IDs with some duplicates\n",
    "    random.seed(42)\n",
    "    ids = [random.randint(1, 100000) for _ in range(100000)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    unique, dups = process_ids(ids)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Found {len(unique)} unique IDs and {dups} duplicates in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "34da7ae64cc3647d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note when I got an AI-generated recommendation, it made some unnecessary code.",
   "id": "a759360453ad9694"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def process_ids(ids):\n",
    "    seen_ids = set()\n",
    "    unique_ids = []\n",
    "    duplicates = 0\n",
    "    for id_val in ids:\n",
    "        if id_val in seen_ids:  # Efficient O(1) lookup on set\n",
    "            duplicates += 1\n",
    "        else:\n",
    "            seen_ids.add(id_val)\n",
    "            unique_ids.append(id_val)  # Maintain order if needed\n",
    "    return unique_ids, duplicates\n",
    "\n",
    "def main():\n",
    "    # Simulate 100,000 IDs with some duplicates\n",
    "    random.seed(42)\n",
    "    ids = [random.randint(1, 100000) for _ in range(100000)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    unique, dups = process_ids(ids)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Found {len(unique)} unique IDs and {dups} duplicates in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "65f36b78a4d80d70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `seen_ids` list was completely redundant to the `seen_ids`.",
   "id": "9506254c69034fb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def process_ids(ids):\n",
    "    unique_ids = set()\n",
    "    duplicates = 0\n",
    "    for id_val in ids:\n",
    "        if id_val in unique_ids:  # Efficient O(1) lookup on set\n",
    "            duplicates += 1\n",
    "        else:\n",
    "            unique_ids.add(id_val)\n",
    "    return unique_ids, duplicates\n",
    "\n",
    "def main():\n",
    "    # Simulate 100,000 IDs with some duplicates\n",
    "    random.seed(42)\n",
    "    ids = [random.randint(1, 100000) for _ in range(100000)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    unique, dups = process_ids(ids)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Found {len(unique)} unique IDs and {dups} duplicates in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "461eb592a556afe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Nested Loop Bottlenecks",
   "id": "6857407f333d51a"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "def process_users(user_data):\n",
    "    # Simulate a large dataset of users\n",
    "    filtered_users = []\n",
    "\n",
    "\n",
    "    for user in user_data:\n",
    "        # Check if user is active and has a valid email\n",
    "        email = user['email']\n",
    "        is_active = user['active']\n",
    "\n",
    "        #  string search for email validation\n",
    "        if is_active and '@' in email:\n",
    "            # checking duplicates\n",
    "            for existing_user in filtered_users:\n",
    "                if existing_user['email'] == email:\n",
    "                    break\n",
    "            else:\n",
    "\n",
    "                processed_user = {\n",
    "                    'email': email.lower() + '_processed',\n",
    "                    'name': user['name'].upper(),\n",
    "                    'id': str(user['id'])\n",
    "                }\n",
    "                filtered_users.append(processed_user)\n",
    "\n",
    "    return filtered_users\n",
    "\n",
    "def main():\n",
    "    # Simulate a large dataset\n",
    "    users = [\n",
    "        {'id': i, 'email': f'user{i}@example.com', 'name': f'User {i}', 'active': i % 2 == 0}\n",
    "        for i in range(100000)\n",
    "    ]\n",
    "\n",
    "    # Process the users\n",
    "    result = process_users(users)\n",
    "    print(f\"Processed {len(result)} users\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "178bcd8f794eff1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def process_users(user_data):\n",
    "    filtered_users = []\n",
    "    seen_emails = set()  # Track duplicates\n",
    "\n",
    "    for user in user_data:\n",
    "        email = user['email']\n",
    "        if user['active'] and '@' in email and email not in seen_emails:\n",
    "            processed_user = {\n",
    "                'email': email.lower() + '_processed',\n",
    "                'name': user['name'].upper(),\n",
    "                'id': str(user['id'])\n",
    "            }\n",
    "            filtered_users.append(processed_user)\n",
    "            seen_emails.add(email)  # Mark as seen\n",
    "\n",
    "    return filtered_users\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Simulate a large dataset\n",
    "    users = [\n",
    "        {'id': i, 'email': f'user{i}@example.com', 'name': f'User {i}', 'active': i % 2 == 0}\n",
    "        for i in range(100000)\n",
    "    ]\n",
    "\n",
    "    # Process the users\n",
    "    result = process_users(users)\n",
    "    print(f\"Processed {len(result)} users\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "c556b8175791e33b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SQL Optimization",
   "id": "a74ada28c5600944"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's start with a common mistake that is easy to do in Python SQL. Let's say I want to insert the next 10_000 dates into a `CALENDAR` table that is a single column of dates. Why is this so slow?",
   "id": "ea350f89841f0e21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T00:10:32.682449Z",
     "start_time": "2025-08-25T00:10:30.503141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sqlite3\n",
    "\n",
    "for _ in range(10_000):\n",
    "    conn = sqlite3.connect(\"company_operations.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"INSERT INTO CALENDAR (CALENDAR_DATE) VALUES\n",
    "                   ((SELECT DATE(MAX(CALENDAR_DATE),'+1 day') FROM CALENDAR))\n",
    "                   \"\"\")\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()"
   ],
   "id": "688239a154939545",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T00:07:28.549737Z",
     "start_time": "2025-08-25T00:07:27.635716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "conn = sqlite3.connect(\"company_operations.db\")"
   ],
   "id": "c928bdd2e524eb43",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's pretend for a moment that this SQL query below took a very long time. Let's see what it recommends.",
   "id": "ee94f0154de942a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T00:07:29.598799Z",
     "start_time": "2025-08-25T00:07:29.592547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sql = \"\"\"\n",
    "SELECT * FROM WEATHER_MONITOR\n",
    "WHERE REPORT_DATE = '2021-05-05'\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "pd.read_sql(sql, conn)\n",
    "end = time.time()\n",
    "print(f\"Query took {end - start:.4f} seconds\")"
   ],
   "id": "d97a7b15ba0d9426",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query took 0.0037 seconds\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "But keep in mind, if the AI recommends an index and other solutions, there are gotchas like write speed going down substantially. These are nuances that you cannot pick up from vibe coding, and is why you should know the subject matter you are prompting.\n",
    "\n",
    "Here is another example. Let's try to optimize this query."
   ],
   "id": "129ff16fcff73e01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"company_operations.db\")\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "CUSTOMER_ID,\n",
    "ORDER_DATE,\n",
    "QUANTITY,\n",
    "(SELECT AVG(QUANTITY)\n",
    " FROM CUSTOMER_ORDER co3\n",
    " WHERE co3.CUSTOMER_ID = co1.CUSTOMER_ID) as avg_customer_quantity\n",
    "FROM CUSTOMER_ORDER co1\n",
    "ORDER BY ORDER_DATE\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "pd.read_sql(sql, conn)\n",
    "end = time.time()\n",
    "print(f\"Query took {end - start:.4f} seconds\")"
   ],
   "id": "b64d096294d4a10f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ideally, we'd like to get a windowing function once the AI recommends a fix. But it may settle for a common table expression or derived table too. This again shows that the AI may propose something that works but may be suboptimal.",
   "id": "c8dce14e27b5673"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vectorization",
   "id": "f9a4ef06c9e8d019"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's see how we can have AI convert our code to be vectorized for numerical operations. This will make our code much faster and help us learn NumPy equivalents to vanilla python loop operations.\n",
    "\n",
    "Here is a simple summation of the numbers 1 through 300 million. It takes nearly 10 seconds to do. Let's see how AI suggests we fix it."
   ],
   "id": "b8a5b0a4ec8d2bcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total = 0\n",
    "for i in range(300_000_000):\n",
    "    total += i\n",
    "\n",
    "print(total)"
   ],
   "id": "a6143393305aad24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here is what ChatGPT suggested I do. It takes less than a second.",
   "id": "aa462ccdf7bccb9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "total = np.sum(np.arange(300_000_000))\n",
    "print(total)"
   ],
   "id": "5decf99c31e94e4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculating this average took nearly 7 seconds.",
   "id": "4241c32629c1346"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = [i for i in range(300_000_000)]\n",
    "\n",
    "mean = sum(x) / len(x)\n",
    "\n",
    "print(\"MEAN: \", mean)"
   ],
   "id": "92d3ad61581fbc41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below, the AI-generated response took less than half a second.",
   "id": "cb8c483111f13467"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(300_000_000)\n",
    "mean = np.mean(x)\n",
    "\n",
    "print(\"MEAN: \", mean)"
   ],
   "id": "d28256dfef19f8fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now here is something painful. This operation takes nearly 30 seconds.",
   "id": "cd1605fa6fcac136"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = list(range(300_000_000))\n",
    "y = list(range(300_000_000))\n",
    "z = []\n",
    "for i in range(len(x)):\n",
    "    z.append(x[i] + y[i])"
   ],
   "id": "6391de5ecc9ecc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "But the AI-generated optimization takes less than 10 seconds.",
   "id": "4998d587712677ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "x = np.arange(300_000_000)\n",
    "y = np.arange(300_000_000)\n",
    "z = x + y"
   ],
   "id": "f8b84d062116c956",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T00:07:47.656128Z",
     "start_time": "2025-08-25T00:07:47.531652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "data = [i * 0.1 for i in range(1_000_000)]  # Simulated dataset\n",
    "mean = sum(data) / len(data)\n",
    "variance = sum((x - mean) ** 2 for x in data) / len(data)\n",
    "std_dev = math.sqrt(variance)\n",
    "normalized = []\n",
    "for val in data:\n",
    "    normalized.append((val - mean) / std_dev)"
   ],
   "id": "5e37d0efece2a151",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T00:07:49.761117Z",
     "start_time": "2025-08-25T00:07:49.747121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "data = np.arange(1_000_000) * 0.1  # Simulated dataset\n",
    "normalized = (data - np.mean(data)) / np.std(data)"
   ],
   "id": "f211bf7435b6e75a",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Computing Pairwise Euclidean Distances for Clustering",
   "id": "2135a580f4010116"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "points = [[i, i+1] for i in range(10_000)]  # List of [x, y] points\n",
    "distances = []\n",
    "for i in range(len(points)):\n",
    "    row = []\n",
    "    for j in range(len(points)):\n",
    "        dx = points[i][0] - points[j][0]\n",
    "        dy = points[i][1] - points[j][1]\n",
    "        row.append(math.sqrt(dx**2 + dy**2))\n",
    "    distances.append(row)"
   ],
   "id": "9ec47def21126a81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "points = np.arange(10_000)[:, np.newaxis] * np.array([1, 1]) + np.array([0, 1])  # Array of shape (10000, 2)\n",
    "diffs = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n",
    "distances = np.sqrt(np.sum(diffs**2, axis=-1))\n",
    "# Alternatively, for even faster: use scipy.spatial.distance.cdist(points, points)"
   ],
   "id": "87085c853049f973",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Filtering and Aggregating Time-Series Data",
   "id": "ccdb329a0ad0e7bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prices = [i * 0.01 for i in range(10_000_000)]  # Time-series prices\n",
    "volumes = [i % 1000 for i in range(10_000_000)]  # Volumes\n",
    "high_volume_prices = []\n",
    "for i in range(len(prices)):\n",
    "    if volumes[i] > 500:\n",
    "        high_volume_prices.append(prices[i])\n",
    "avg_price = sum(high_volume_prices) / len(high_volume_prices) if high_volume_prices else 0"
   ],
   "id": "83186b7a83382caa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "prices = np.arange(10_000_000) * 0.01\n",
    "volumes = np.arange(10_000_000) % 1000\n",
    "mask = volumes > 500\n",
    "high_volume_prices = prices[mask]\n",
    "avg_price = np.mean(high_volume_prices) if len(high_volume_prices) > 0 else 0"
   ],
   "id": "229df3a1e1c5abac",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
